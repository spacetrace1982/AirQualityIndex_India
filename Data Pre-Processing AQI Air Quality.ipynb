{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b668d53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes with the most missing values:\n",
      "Xylene        18109\n",
      "PM10          11140\n",
      "NH3           10328\n",
      "Toluene        8041\n",
      "Benzene        5623\n",
      "AQI            4681\n",
      "AQI_Bucket     4681\n",
      "PM2.5          4598\n",
      "NOx            4185\n",
      "O3             4022\n",
      "SO2            3854\n",
      "NO2            3585\n",
      "NO             3582\n",
      "CO             2059\n",
      "City              0\n",
      "Date              0\n",
      "dtype: int64\n",
      "\n",
      "Cities with 90-100% completeness:\n",
      "City\n",
      "Aizawl                98.910824\n",
      "Amaravati             93.763650\n",
      "Amritsar              92.389592\n",
      "Bengaluru             95.849447\n",
      "Bhopal                97.258451\n",
      "Chandigarh            98.810729\n",
      "Coimbatore            94.001594\n",
      "Delhi                 98.828349\n",
      "Guwahati              99.662887\n",
      "Hyderabad             95.390751\n",
      "Jaipur                98.225383\n",
      "Kochi                 98.480532\n",
      "Kolkata               96.276696\n",
      "Thiruvananthapuram    96.755672\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Function created to assess the data from kaggle.\n",
    "\n",
    "def process_data(file_path):\n",
    "    # Load the original data- load the csv file from its raw status\n",
    "    original_data = pd.read_csv(file_path)\n",
    "\n",
    "    # Identify attributes with the most missing values. \n",
    "    missing_data = original_data.isnull().sum().sort_values(ascending=False)\n",
    "    print(\"Attributes with the most missing values:\")\n",
    "    print(missing_data)\n",
    "\n",
    "    # Remove \"Xylene\", \"Toluene\", and \"Benzene\" and save to a new file. These compounds are not used in the AQI calculation\n",
    "    data_without_xtb = original_data.drop(columns=['Xylene', 'Toluene', 'Benzene'])\n",
    "    data_without_xtb.to_csv('AQI_Data_First_Cleanse.csv', index=False)\n",
    "\n",
    "    # Calculate completeness of dataset for each city. \n",
    "    city_completeness = data_without_xtb.groupby('City').apply(lambda group: group.notnull().sum().sum() / group.size)\n",
    "    city_completeness_percent = city_completeness * 100\n",
    "\n",
    "    # Identify cities with 90-100% completeness - Select cities that are >90% complete\n",
    "    top_cities = city_completeness_percent[city_completeness_percent >= 90].index\n",
    "\n",
    "    # Print the cities with their completeness percentages - show the results to make decision: how many cities are left?\n",
    "    print(\"\\nCities with 90-100% completeness:\")\n",
    "    print(city_completeness_percent[city_completeness_percent >= 90])\n",
    "\n",
    "    # Create a dataset of only the cities with >=90% completeness - only >90% included: adjust above %if needed.\n",
    "    data_top_cities = data_without_xtb[data_without_xtb['City'].isin(top_cities)]\n",
    "\n",
    "    # Remove rows with missing 'AQI' and 'AQI_Bucket'- remove any AQI or AQI buckets for training model\n",
    "    data_top_cities = data_top_cities.dropna(subset=['AQI', 'AQI_Bucket'])\n",
    "\n",
    "    # Save the final dataset - this is the dataset to take into WEKA\n",
    "    data_top_cities.to_csv('Cities_Above_90_Completeness.csv', index=False)\n",
    "\n",
    "    # Return the cleaned data for further processing (like outlier detection)\n",
    "    return data_top_cities\n",
    "\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'city_day.csv'  # please replace with your actual file path\n",
    "\n",
    "# Run the data processing function\n",
    "cleaned_data = process_data(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e876c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P-values from Shapiro-Wilk test:\n",
      "PM2.5: 0.0\n",
      "PM10: 0.0\n",
      "NO: 0.0\n",
      "NO2: 0.0\n",
      "NOx: 0.0\n",
      "NH3: 0.0\n",
      "CO: 0.0\n",
      "SO2: 0.0\n",
      "O3: 0.0\n",
      "AQI: 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import shapiro\n",
    "import numpy as np\n",
    "\n",
    "#function created to assess whether the attributes are normally distributed. \n",
    "\n",
    "def test_normality(data):\n",
    "    # Results will be stored in a dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Select only numeric columns\n",
    "    numeric_columns = data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Perform Shapiro-Wilk test for each numeric column\n",
    "    for col in numeric_columns.columns:\n",
    "        _, p_value = shapiro(data[col].dropna())  # dropna to remove missing values\n",
    "        results[col] = p_value\n",
    "    \n",
    "    # Print the p-values\n",
    "    print(\"\\nP-values from Shapiro-Wilk test:\")\n",
    "    for col, p_value in results.items():\n",
    "        print(f\"{col}: {p_value}\")\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'Cities_Above_90_Completeness.csv'  #  replace with your actual file path\n",
    "\n",
    "\n",
    "# Call the test_normality function and print the results\n",
    "test_normality(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c972509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of outliers for each attribute:\n",
      "PM2.5     912\n",
      "PM10      765\n",
      "NO       1361\n",
      "NO2       352\n",
      "NOx       934\n",
      "NH3       410\n",
      "CO        760\n",
      "SO2       437\n",
      "O3        312\n",
      "AQI      1164\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#function to calculate outliers based on IQR\n",
    "\n",
    "def calculate_outliers(df):\n",
    "    # Selecting only numeric columns to calculate outliers\n",
    "    numeric_columns = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calculating IQR for each column\n",
    "    Q1 = numeric_columns.quantile(0.25)\n",
    "    Q3 = numeric_columns.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Determining upper and lower bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Counting the number of outliers in each column\n",
    "    return ((numeric_columns < lower_bound) | (numeric_columns > upper_bound)).sum()\n",
    "\n",
    "# Call the calculate_outliers function separately and print the results\n",
    "print(\"\\nNumber of outliers for each attribute:\")\n",
    "print(calculate_outliers(cleaned_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cab892ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the outliers and write to a newcsv file \n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Selecting only numeric columns to calculate outliers\n",
    "    numeric_columns = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calculating IQR for each column\n",
    "    Q1 = numeric_columns.quantile(0.25)\n",
    "    Q3 = numeric_columns.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Determining upper and lower bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filtering out the outliers and keeping only valid values\n",
    "    no_outliers = df[~((numeric_columns < lower_bound) | (numeric_columns > upper_bound)).any(axis=1)]\n",
    "\n",
    "    return no_outliers\n",
    "\n",
    "# Remove outliers and save the resulting DataFrame to 'data_outliers_removed.csv'\n",
    "data_without_outliers = remove_outliers(cleaned_data)\n",
    "data_without_outliers.to_csv('data_outliers_removed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82392cbf",
   "metadata": {},
   "source": [
    "\"\"\"\"The section below is for the  handling of null values and normalising the datasets for those numeric clumns which are not the AQI index. The method for the normalisation was to apply the average value and the normalisation was based on a scaller method to create values between 0-1\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48996aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed and normalized data saved to imputed_normalized_data_nooutliers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trace\\AppData\\Local\\Temp\\ipykernel_45748\\3568117160.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  data.fillna(data.mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#It is important to check that the correct files are being used to normalise and impute in this function. \n",
    "def impute_and_normalize(file_path):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Impute missing values with mean\n",
    "    data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "    # Select columns to normalize (numeric columns excluding 'AQI') Do not want to normalise the data for the AQI result\n",
    "    cols_to_normalize = data.select_dtypes(include=[float]).columns.tolist()\n",
    "    cols_to_normalize.remove('AQI')\n",
    "\n",
    "    # Initialize a scaler, then apply it to the columns: Applying the normalisation to the dataset base on scaling 0-1\n",
    "    scaler = MinMaxScaler()\n",
    "    data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "    # Save the imputed and normalized dataset to a new CSV file\n",
    "    output_file = 'imputed_normalized_data_nooutliers.csv'#change the file name output as required for different datasets\n",
    "    data.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Imputed and normalized data saved to {output_file}\") #Check this before completing\n",
    "\n",
    "# Replace the 'filename'as appropriate to normalise the correct dataset - with or without the outliers.\n",
    "impute_and_normalize('data_outliers_removed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c216bf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness of the dataset: 98.34%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Function to check the overall completeness of any csv files that is selected.\n",
    "def calculate_completeness(file_path):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Calculate the total number of cells in the dataframe\n",
    "    total_cells = np.product(data.shape)\n",
    "    \n",
    "    # Count the number of missing values per column\n",
    "    missing_counts = data.isnull().sum()\n",
    "    \n",
    "    # Count the total number of missing values\n",
    "    total_missing = missing_counts.sum()\n",
    "    \n",
    "    # Calculate completeness\n",
    "    completeness = ((total_cells - total_missing) / total_cells) * 100\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Completeness of the dataset: {completeness:.2f}%\")\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "#file_path = 'your_data.csv'  # please replace with your actual file path if needed.\n",
    "\n",
    "# Call the function - change the file name below as required to call the function on the correct file. \n",
    "calculate_completeness('Cities_Above_90_Completeness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7bd46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
