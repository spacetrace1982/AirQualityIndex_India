{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c870fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Category  Missing Values  Percentage Missing (%)\n",
      "13      Xylene           18109               61.322001\n",
      "3         PM10           11140               37.723071\n",
      "7          NH3           10328               34.973418\n",
      "12     Toluene            8041               27.229014\n",
      "11     Benzene            5623               19.041008\n",
      "14         AQI            4681               15.851139\n",
      "15  AQI_Bucket            4681               15.851139\n",
      "2        PM2.5            4598               15.570079\n",
      "6          NOx            4185               14.171549\n",
      "10          O3            4022               13.619586\n",
      "9          SO2            3854               13.050692\n",
      "5          NO2            3585               12.139785\n",
      "4           NO            3582               12.129626\n",
      "8           CO            2059                6.972334\n",
      "0         City               0                0.000000\n",
      "1         Date               0                0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_missing_data(city_day):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(city_day)\n",
    "\n",
    "    # Calculate the total number of entries\n",
    "    total_entries = len(data)\n",
    "\n",
    "    # Calculate the number of missing values in each column\n",
    "    missing_data = data.isnull().sum()\n",
    "\n",
    "    # Calculate the percentage of missing values\n",
    "    missing_data_percent = (missing_data / total_entries) * 100\n",
    "\n",
    "    # Create a dataframe to hold the results\n",
    "    missing_data_df = pd.DataFrame({\n",
    "        'Category': missing_data.index,\n",
    "        'Missing Values': missing_data.values,\n",
    "        'Percentage Missing (%)': missing_data_percent.values\n",
    "    })\n",
    "\n",
    "    # Sort the results by the number of missing values in descending order\n",
    "    missing_data_df = missing_data_df.sort_values(by='Missing Values', ascending=False)\n",
    "\n",
    "    return missing_data_df\n",
    "\n",
    "# Specify the name of your CSV file\n",
    "file_name = 'city_day.csv'  # please make sure this file is in the same directory as your script\n",
    "\n",
    "# Perform the analysis\n",
    "missing_data_analysis = analyze_missing_data(file_name)\n",
    "\n",
    "# Display the results\n",
    "print(missing_data_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb3cbdbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Drop the 'Xylene' column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_without_xylene \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXylene\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate the completeness for each city\u001b[39;00m\n\u001b[1;32m      5\u001b[0m completeness \u001b[38;5;241m=\u001b[39m data_without_xylene\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m group: group\u001b[38;5;241m.\u001b[39mnotnull()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m group\u001b[38;5;241m.\u001b[39msize)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Drop the 'Xylene' column\n",
    "    data_without_xylene = data.drop(columns=['Xylene'])\n",
    "\n",
    "    # Calculate the completeness for each city\n",
    "    completeness = data_without_xylene.groupby('City').apply(lambda group: group.notnull().sum().sum() / group.size)\n",
    "\n",
    "    # Convert to percentage\n",
    "    completeness_percent = completeness * 100\n",
    "\n",
    "    # Sort cities by completeness in descending order\n",
    "    most_complete_cities = completeness_percent.sort_values(ascending=False)\n",
    "\n",
    "    return most_complete_cities\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'path_to_your_file.csv'  # please replace with your actual file path\n",
    "\n",
    "# Calculate the completeness\n",
    "completeness_results = calculate_completeness(file_path)\n",
    "\n",
    "# Display the results\n",
    "print(completeness_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aeb8b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Data Analysis:\n",
      "      Category  Missing Values  Percentage Missing (%)\n",
      "13      Xylene           18109               61.322001\n",
      "3         PM10           11140               37.723071\n",
      "7          NH3           10328               34.973418\n",
      "12     Toluene            8041               27.229014\n",
      "11     Benzene            5623               19.041008\n",
      "14         AQI            4681               15.851139\n",
      "15  AQI_Bucket            4681               15.851139\n",
      "2        PM2.5            4598               15.570079\n",
      "6          NOx            4185               14.171549\n",
      "10          O3            4022               13.619586\n",
      "9          SO2            3854               13.050692\n",
      "5          NO2            3585               12.139785\n",
      "4           NO            3582               12.129626\n",
      "8           CO            2059                6.972334\n",
      "0         City               0                0.000000\n",
      "1         Date               0                0.000000\n",
      "\n",
      "Completeness Analysis (without 'Xylene'):\n",
      "City\n",
      "Delhi                 98.984569\n",
      "Chandigarh            98.969298\n",
      "Jaipur                98.192699\n",
      "Aizawl                97.286136\n",
      "Kolkata               96.756757\n",
      "Bengaluru             95.211548\n",
      "Hyderabad             95.161183\n",
      "Coimbatore            94.697755\n",
      "Amaravati             93.543638\n",
      "Guwahati              92.948207\n",
      "Kochi                 92.016461\n",
      "Amritsar              91.182091\n",
      "Visakhapatnam         89.311446\n",
      "Chennai               89.075825\n",
      "Ernakulam             88.847737\n",
      "Lucknow               86.484155\n",
      "Bhopal                84.290657\n",
      "Thiruvananthapuram    83.854916\n",
      "Shillong              83.591398\n",
      "Gurugram              79.023228\n",
      "Patna                 78.320775\n",
      "Talcher               77.736937\n",
      "Brajrajnagar          76.240227\n",
      "Ahmedabad             65.820475\n",
      "Jorapokhar            57.604790\n",
      "Mumbai                56.173884\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_data(city_day):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(city_day)\n",
    "\n",
    "    # Calculate the total number of entries\n",
    "    total_entries = len(data)\n",
    "\n",
    "    # Calculate the number of missing values in each column\n",
    "    missing_data = data.isnull().sum()\n",
    "\n",
    "    # Calculate the percentage of missing values\n",
    "    missing_data_percent = (missing_data / total_entries) * 100\n",
    "\n",
    "    # Create a dataframe to hold the results\n",
    "    missing_data_df = pd.DataFrame({\n",
    "        'Category': missing_data.index,\n",
    "        'Missing Values': missing_data.values,\n",
    "        'Percentage Missing (%)': missing_data_percent.values\n",
    "    })\n",
    "\n",
    "    # Sort the results by the number of missing values in descending order\n",
    "    missing_data_df = missing_data_df.sort_values(by='Missing Values', ascending=False)\n",
    "\n",
    "    # Analyze completeness per city after removing 'Xylene'\n",
    "    data_without_xylene = data.drop(columns=['Xylene'])\n",
    "\n",
    "    # Calculate the completeness for each city\n",
    "    completeness = data_without_xylene.groupby('City').apply(lambda group: group.notnull().sum().sum() / group.size)\n",
    "\n",
    "    # Convert to percentage\n",
    "    completeness_percent = completeness * 100\n",
    "\n",
    "    # Sort cities by completeness in descending order\n",
    "    most_complete_cities = completeness_percent.sort_values(ascending=False)\n",
    "\n",
    "    return missing_data_df, most_complete_cities\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'city_day.csv'  # please make sure this file is in the same directory as your script\n",
    "\n",
    "# Perform the analysis - Completeness of data after Xylene is removed\n",
    "missing_data_analysis, completeness_results = analyze_data(file_path)\n",
    "\n",
    "# Display the results\n",
    "print(\"Missing Data Analysis:\")\n",
    "print(missing_data_analysis)\n",
    "print(\"\\nCompleteness Analysis (without 'Xylene'):\")\n",
    "print(completeness_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f94ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness of the dataset after first cleanse: 85.22673696220139%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_and_save_data(city_day):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(city_day)\n",
    "\n",
    "    # Remove the specified columns\n",
    "    columns_to_remove = ['Xylene', 'Toluene', 'Benzene']\n",
    "    data_cleaned = data.drop(columns=columns_to_remove)\n",
    "\n",
    "    # Calculate the completeness of the dataset\n",
    "    completeness = data_cleaned.notnull().sum().sum() / data_cleaned.size * 100\n",
    "\n",
    "    # Save the cleaned data to a new CSV file\n",
    "    data_cleaned.to_csv('AQI_Data_First_Cleanse.csv', index=False)\n",
    "\n",
    "    return completeness\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'city_day.csv'  # please replace with your actual file path\n",
    "\n",
    "# Clean the data and calculate completeness\n",
    "completeness_after_cleanse = clean_and_save_data(file_path)\n",
    "\n",
    "# Print the completeness\n",
    "print(f\"Completeness of the dataset after first cleanse: {completeness_after_cleanse}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ded25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to AQI_Data_Second_Cleanse.csv after removing PM10.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_pm10_and_save(file_path, output_file):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Remove the 'PM10' column\n",
    "    data_without_pm10 = data.drop(columns=['PM10'])\n",
    "\n",
    "    # Save the modified data to a new CSV file\n",
    "    data_without_pm10.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Data saved to {output_file} after removing PM10.\")\n",
    "\n",
    "# Specify the path to your input CSV file and the desired output file name\n",
    "input_file_path = 'AQI_Data_First_Cleanse.csv'  # make sure this file is in your script's directory\n",
    "output_file = 'AQI_Data_Second_Cleanse.csv'  # name of the new file after removing PM10\n",
    "\n",
    "# Remove PM10 and save to a new file\n",
    "remove_pm10_and_save(input_file_path, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db33605",
   "metadata": {},
   "source": [
    "### Dont use the code above : this is now amalgamated to the code below. Sanity check and mark it up correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a9f14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes with the most missing values:\n",
      "Xylene        18109\n",
      "PM10          11140\n",
      "NH3           10328\n",
      "Toluene        8041\n",
      "Benzene        5623\n",
      "AQI            4681\n",
      "AQI_Bucket     4681\n",
      "PM2.5          4598\n",
      "NOx            4185\n",
      "O3             4022\n",
      "SO2            3854\n",
      "NO2            3585\n",
      "NO             3582\n",
      "CO             2059\n",
      "City              0\n",
      "Date              0\n",
      "dtype: int64\n",
      "\n",
      "Cities with 90-100% completeness:\n",
      "City\n",
      "Aizawl                98.910824\n",
      "Amaravati             93.763650\n",
      "Amritsar              92.389592\n",
      "Bengaluru             95.849447\n",
      "Bhopal                97.258451\n",
      "Chandigarh            98.810729\n",
      "Coimbatore            94.001594\n",
      "Delhi                 98.828349\n",
      "Guwahati              99.662887\n",
      "Hyderabad             95.390751\n",
      "Jaipur                98.225383\n",
      "Kochi                 98.480532\n",
      "Kolkata               96.276696\n",
      "Thiruvananthapuram    96.755672\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_data(file_path):\n",
    "    # Load the original data\n",
    "    original_data = pd.read_csv(file_path)\n",
    "\n",
    "    # Identify attributes with the most missing values\n",
    "    missing_data = original_data.isnull().sum().sort_values(ascending=False)\n",
    "    print(\"Attributes with the most missing values:\")\n",
    "    print(missing_data)\n",
    "\n",
    "    # Remove \"Xylene\", \"Toluene\", and \"Benzene\" and save to a new file\n",
    "    data_without_xtb = original_data.drop(columns=['Xylene', 'Toluene', 'Benzene'])\n",
    "    data_without_xtb.to_csv('AQI_Data_First_Cleanse.csv', index=False)\n",
    "\n",
    "    # Calculate completeness for each city\n",
    "    city_completeness = data_without_xtb.groupby('City').apply(lambda group: group.notnull().sum().sum() / group.size)\n",
    "    city_completeness_percent = city_completeness * 100\n",
    "\n",
    "    # Identify cities with 90-100% completeness\n",
    "    top_cities = city_completeness_percent[city_completeness_percent >= 90].index\n",
    "\n",
    "    # Print the cities with their completeness percentages\n",
    "    print(\"\\nCities with 90-100% completeness:\")\n",
    "    print(city_completeness_percent[city_completeness_percent >= 90])\n",
    "\n",
    "    # Create a dataset of only the cities with >=90% completeness\n",
    "    data_top_cities = data_without_xtb[data_without_xtb['City'].isin(top_cities)]\n",
    "\n",
    "    # Remove rows with missing 'AQI' and 'AQI_Bucket'\n",
    "    data_top_cities = data_top_cities.dropna(subset=['AQI', 'AQI_Bucket'])\n",
    "\n",
    "    # Save the dataset with 'PM10'\n",
    "    data_top_cities.to_csv('Cities_Above_90_With_PM10.csv', index=False)\n",
    "\n",
    "    # Remove 'PM10' and save the dataset\n",
    "    data_top_cities_without_pm10 = data_top_cities.drop(columns=['PM10'])\n",
    "    data_top_cities_without_pm10.to_csv('Cities_Above_90_Without_PM10.csv', index=False)\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'city_day.csv'  # please replace with your actual file path\n",
    "\n",
    "# Run the data processing function\n",
    "process_data(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2461d4",
   "metadata": {},
   "source": [
    "The data has been cleansed as per the following method: \n",
    "The dataset first comprised on 16 attributes with 29,531 entries. 78.88% of the data had missing values across one or multiple attributes which would severely impact the performance and reliability of any machine learning models trained on the remaining data. On further analysis by attributes there was significant data missing for Toulene, Benzene and Xylene- these were moved from the analysis as literature search (references)  has suggested that these attributes do not contibute to the overall AQI. \n",
    "\n",
    "To not bias the data significantly by imputation methods or by deleting too many empty values. The dataset was assessed per city for completness of data. The intention was to have a broad range of cities included in the overall analysis as many other literature search focused on one or two cities alone and had limited datasets contributing the overal ML models. The dataset was analysed by  determining the cities where completness of dataset of >90% were selected, this resulted in 14 cities being included in the final analysis. \n",
    "\n",
    "Finally 891 entries were removed based on not having a final computed AQI value or bucket. This gave a final data set where 14 cities  controbutes >95% completeness to the data, all records reported a final AQI value and a total number of entries weew 17,010 with 13 attributes.\n",
    "\n",
    "A secondary dataset was created with 12 attributes and this was with the pm10 removed for the pusposes of a comparision between pm10  included and or excluded to see if it signifciatnly contributed to the overal AQI prediction. This dataset demonstrates an overall completeness of \"Cities_Above_90_Without_PM10.csv\" dataset is approximately 98.63%. This indicates a high level of completeness, with most of the data available for analysis or modeling purposes without imputation and limits the overall bias introduced. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
